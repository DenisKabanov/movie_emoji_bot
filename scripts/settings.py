#===============================================================================
# файл, содержащий важные параметры
#===============================================================================

# общие настройки
DATA_DIR = "./data/"
CONVERTATION_MODELS_DIR = "./models/convertation/"
SUMMARIZATION_MODELS_DIR = "./models/summarization/"
MODEL_NAME = "t5_finetuned" # название сохранённой модели (папки с моделью)


# настройки для обучения
# RANDOM_SEED = 42 # используемый seed для рандомизации при обучении
TRAIN_DATASET_SIZE = 10000 # размер датасета для обучения
TEST_DATASET_SIZE = 1000 # размер датасета для оценивания
CHECKPOINT = None # путь до чекпоинта, с которого модель продолжит обучение (None — обучение с "нуля")
NUM_BEAMS = 5 # Number of beams for beam search that will be used by default in the generate
MAX_TARGET_TOKEN_COUNT = 200 # максимальное число токенов в ответе модели
MAX_SOURCE_TOKEN_COUNT = 600 # максимальное число токенов на входе для модели
# OUTPUT_DIR = SUMMARIZATION_MODELS_DIR + MODEL_NAME + "/training/" # путь, по которому будут сохраняться чекпоинты (и предсказания) во время обучения (ПОЛНЫЙ путь не должен включать кириллицу)
OUTPUT_DIR = "C:/Users/User/Desktop/t5/training" # путь, по которому будут сохраняться чекпоинты (и предсказания) во время обучения (ПОЛНЫЙ путь не должен включать кириллицу)
OVERWRITE_OUTPUT_DIR = False # перезаписывать ли содержимое папки OUTPUT_DIR, если имена будут совпадать
SAVE_LIMIT = None # сколько максимум хранить чекпоинтов в папке, старые будут просто перезаписываться (None — без лимита)
TRAIN_EPOCHS = 5 # число эпох для обучения
LEARNING_RATE = 0.0005 # размер шага обучения
GRADIENT_ACCUMULATION_STEPS = 1 # число итераций, за которые будет накапливаться градиент для backward pass (обновления параметров модели)
BATCH_SIZE = 8 # размер батча (число сэмлов, передающееся в модель за одну итерацию)
EVAL_STRATEGY = "steps" # как часто оценивать качество обучения (возможные значения: "no", "steps", "epoch")
EVAL_STEPS = 200 # через сколько шагов обновления проводить оценку качества (только при использовании EVAL_STRATEGY="steps")
SAVE_STRATEGY="steps" # как часто сохранять чекпоинты обучения (возможные значения: "no", "steps", "epoch")
SAVE_STEPS = 200 # через сколько шагов обновления сохранять чекпоинт (только при использовании SAVE_STRATEGY="steps")
LOGGING_STRATEGY = "steps" # как часто выводить лог обучения (возможные значения: "no", "steps", "epoch")
LOGGING_STEPS = 200 # через сколько шагов обновления выводить лог (только при использовании EVAL_STRATEGY="steps")
WARMUP_STEPS = 100 # количество шагов, используемое для линейного увеличения learning_rate от 0 до LEARNING_RATE
FP_16 = False # работать ли с fp16 вместо fp32
PREDICT_WITH_GENERATE = True # использовать ли метод generate для подсчёта генеративных метрик (ROUGE, BLEU)
LOAD_BEST_MODEL_AT_END = True # возвращать ли лучшую модель в конце обучения
METRIC_FOR_BEST_MODEL = "bleu" # название метрики, по которой будет определяться лучшая модель
GREATER_BETTER = True # для LOAD_BEST_MODEL_AT_END и METRIC_FOR_BEST_MODEL, чтобы определить, должна ли метрика увеличиваться


# настройки для инференса суммаризационной модели
TEXT_TO_SUMMARIZE = "Зима в этом году в тридевятом царстве случилась особенно долгая и холодная. Прямо, как в прошлом году. И как в позапрошлом. Но, вот, пришла весна. Точнее – почти пришла. С вьюгой, морозами и снегом. Как обычно. И совсем было загрустили наши герои – Иван, Василиса, Серый Волк и Царь-батюшка, если бы вместе с почти-весной, не пришла новость - в Трисемнадцатом царстве пройдет ежегодный всесказочный конкурс песни. В прошлом году там победил хомяк Жорж, а чей голос окажется круче на этот раз? Царь решил, что Тридевятое царство будет представлять Кот-ученый. Не один все же мультфильм прошли вместе, да и все равно он про книги забыл, орет, точнее поет целыми днями на крышах, как и положено коту в марте. И, конечно, вся компания во главе с Иваном и Волком отправляется поддержать пушистого певца. Ведь свои нужны рядом. А то шоу-бизнес – настоящий серпентарий, и кто знает, какие темные силы стоят за коварными соперниками и их стремлением стать звездой. Кот ученый и друзья едут за тридевять земель на конкурс песни. Музыкальная сказка о волшебном мире шоу-бизнеса." # текст для суммаризации
NO_REPEAT_NGRAM_SIZE = 4 # n-grams данного размера могут возникнуть лишь раз
MAX_LENGTH = 600 # максимальное число токенов, в которые может быть преобразован TEXT_TO_SUMMARIZE


# настройки для преобразования текста в эмодзи
CONV_NAME = "movies_t5_finetuned" # название json файла, что нужно сконвертировать в эмодзи ("movies_base", "movies_t5", "movies_t5_finetuned"...)
ADD_STOP_WORDS = ["тот"] # дополнительные стоп-слова
MIN_SIMILARITY = 0.76 # минимальная косинусная близость между словом и эмодзи, чтобы добавить эмодзи к мультфильму
MIN_EMOJIS = 2 # минимальное число эмодзи у мультфильма, чтобы оставить его в датасете


# настройки для бота
DATA_FILE = "movies_comb_emoj.json" # название файла, из которогу будут использоваться данные в боте (должно включать '_emoj', так как эмодзи есть только в этих файлах)
